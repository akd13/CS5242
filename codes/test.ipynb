{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "*Changelog*:\n",
    "\n",
    "v1: correct the deadline, which should be 11 Mar. \n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "\n",
    "**ASSIGNMENT DEADLINE: 11 Mar 2019 (MON) 17:00**\n",
    "\n",
    "In this assignemnt, the task is to implement some basic components for training convolutional neural network over the MNIST dataset. Firstly, you need to implement the adam algorithm in nn/optimizers.py. Secondly, you need to implement the operations in nn/operations.py, which are used by the layers (in nn/layer.py). Finally, you need to tune the model structure and some hyperparameters to improve the accuracy.\n",
    "\n",
    "**Attention**:\n",
    "- *Only python3 is allowed to use in this assignment.*\n",
    "- *`numpy` is utilized for computation.*\n",
    "- *You do not need a GPU to for this assignment. CPU is enough.*\n",
    "- *To run this Jupyter notebook, you need to install the depedent libraries as stated in [README.MD](README.MD).*\n",
    "- *Please do not run this whole file before you implement all the codes. Otherwise some errors will occur.*\n",
    "- *Please do not change the inputs arguments of the functions in py files, otherwise your implementation would fail to pass the test.*\n",
    "- *After you implement one function, remember to restart the notebook kernel to help it recognize your fresh code.*\n",
    "- *Please do not change the structure of files in the whole folder of this assignment, otherwise TA may mark your code wrongly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Structure of Codes\n",
    "\n",
    "The structure of provided codes and the functionality of its containing files is shown as below:\n",
    "\n",
    "```bash\n",
    "codes/\n",
    "    data/\n",
    "        mnist.npz       # mnist dataset \n",
    "    models/    # example models of your tiny deep learning framework\n",
    "        MNISTNet.py     # example model on MNIST dataset\n",
    "    nn/        # components of neural networks\n",
    "        layers.py       # layer abstract for CNN\n",
    "        loss.py         # loss function for optimization\n",
    "        model.py        # model abstraction for defining and training models\n",
    "        operations.py   # operation abstraction for defined layers, your main workspace\n",
    "        optimizers.py   # optimizing methods, you only need to implement Adam\n",
    "    utils/     # some additional tools for CNN\n",
    "        check_grads.py  # help you check whether your forward function and backward function are consistent\n",
    "        datasets.py     # load dataset, like MNIST\n",
    "        initializers.py # initializing methods to initialize parameters (like weights, bias)\n",
    "        tools.py        # other useful functions\n",
    "    main.ipynb # interactive notebook, help you understand your task\n",
    "    README.MD  # requirements to run main.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Functionality of This Notebook\n",
    "\n",
    "This iPython notebook serves to:\n",
    "\n",
    "- explain code structure, main APIs and implementation examples (`FC`) \n",
    "- explain your task\n",
    "- provide code to test your implemented forward and backward function for different operations\n",
    "- provide related materials to help you understand the implementation of some operations and optimizers\n",
    "\n",
    "*You can type `jupyter lab` in the terminal to start this jupyter notebook while you are in the folder containing this file. It's much more convinient than jupyter notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer\n",
    "In the file [nn/optimizers.py](nn/optimizers.py), there are 4 types of optimizer (`SGD`, `Adam`, `RMSprop` and `Adagrad`). **You only need to implement the `update` function of `Adam`**. \n",
    "\n",
    "`Adam` optimizer is initialized like this:\n",
    "\n",
    "```python\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0, sheduler_func=None):\n",
    "        super(Adam, self).__init__(lr)\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        if not self.epsilon:\n",
    "            self.epsilon = 1e-8\n",
    "        self.moments = None\n",
    "        self.accumulators = None\n",
    "        self.sheduler_func = sheduler_func\n",
    "```\n",
    "\n",
    "- `lr`: The initial learning rate.\n",
    "- `decay`: The learning rate decay ratio\n",
    "- `sheduler_func`: Function to change learning rate with respect to iterations\n",
    "\n",
    "More details can be seen in the reference.\n",
    "\n",
    "*For your reference:* http://cs231n.github.io/neural-networks-3/#update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covolution Layer\n",
    "In the file [nn/layers.py](nn/layers.py), the initialization, forward and backward function of the class `Convolution` are shown as below:\n",
    "```python\n",
    "class Convolution(Layer):\n",
    "    def __init__(self, conv_params, initializer=Gaussian(), name='conv'):\n",
    "        super(Convolution, self).__init__(name=name)\n",
    "        self.trainable = True\n",
    "        self.conv_params = conv_params\n",
    "        self.conv = conv(conv_params)\n",
    "\n",
    "        self.weights = initializer.initialize(\n",
    "            (conv_params['out_channel'], conv_params['in_channel'], conv_params['kernel_h'], conv_params['kernel_w']))\n",
    "        self.bias = np.zeros((conv_params['out_channel']))\n",
    "\n",
    "        self.w_grad = np.zeros(self.weights.shape)\n",
    "        self.b_grad = np.zeros(self.bias.shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv.forward(input, self.weights, self.bias)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        in_grad, self.w_grad, self.b_grad = self.conv.backward(\n",
    "            out_grad, input, self.weights, self.bias)\n",
    "        return in_grad\n",
    "\n",
    "```\n",
    "\n",
    "`conv_params` is a dictionary, containing these parameters:\n",
    "\n",
    "- 'kernel_h': The height of kernel.\n",
    "- 'kernel_w': The width of kernel.\n",
    "- 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions.\n",
    "- 'pad': The number of pixels padded to the bottom, top, left and right of each feature map. **Here pad=2 means adding 2 zeros to the left, right, top and bottom respectively**.\n",
    "- 'in_channel': The number of input channels.\n",
    "- 'out_channel': The number of output channels.\n",
    "\n",
    "`initializer` is an instance of Initializer class, used to initialize parameters\n",
    "\n",
    "You need to implement forward and backward funtion of `conv` class in [nn/operations.py](nn/operations.py), which are called in the `Convolution` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward\n",
    "In the file [nn/operations.py](nn/operations.py), implement the forward function for `conv` class.\n",
    "\n",
    "The input consists of N data points, each with C channels, height H and width W. We convolve each input with K different kernels, where each filter spans all C channels and has height HH and width WW.\n",
    "\n",
    "**WARNING: Please implement the matrix product method of convolution as shown in Lecture notes. The naive version of implementing a sliding window will be too slow when you try to train the whole CNN in later sections.**\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Relative error (<1e-6 will be fine):  3.745891196121655e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from nn.layers import Convolution\n",
    "from utils.tools import rel_error\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "input = np.random.uniform(size=(10, 3, 30, 30))\n",
    "params = { \n",
    "    'kernel_h': 5,\n",
    "    'kernel_w': 5,\n",
    "    'pad': 0,\n",
    "    'stride': 2,\n",
    "    'in_channel': input.shape[1],\n",
    "    'out_channel': 64,\n",
    "}\n",
    "conv = Convolution(params)\n",
    "out = conv.forward(input)\n",
    "keras_conv = Sequential([\n",
    "    Conv2D(filters=params['out_channel'],\n",
    "            kernel_size=(params['kernel_h'], params['kernel_w']),\n",
    "            strides=(params['stride'], params['stride']),\n",
    "            padding='valid',\n",
    "            data_format='channels_first',\n",
    "            input_shape=input.shape[1:]),\n",
    "])\n",
    "keras_conv.layers[0].set_weights([conv.weights.transpose((2,3,1,0)), conv.bias])\n",
    "keras_out = keras_conv.predict(input, batch_size=input.shape[0])\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward\n",
    "Implement the backward function for the `conv` class in the file [nn/operations.py](nn/operations.py). \n",
    "\n",
    "When you are done, restart jupyter notebook and run the following to check your backward pass with a numeric gradient check. \n",
    "\n",
    "In gradient checking, to get an approximate gradient for a parameter, we vary that parameter by a small amount (while keeping rest of parameters constant) and note the difference in the network loss. Dividing the difference in network loss by the amount we varied the parameter gives us an approximation for the gradient. We repeat this process for all the other parameters to obtain our numerical gradient. Note that gradient checking is a slow process (2 forward propagations per parameter) and should only be used to check your backpropagation!\n",
    "\n",
    "More links on gradient checking:\n",
    "\n",
    "http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient to input: correct\n",
      "Gradient to weights:  correct\n",
      "Gradient to bias:  correct\n"
     ]
    }
   ],
   "source": [
    "from nn.layers import Convolution\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "batch = 10\n",
    "conv_params={\n",
    "    'kernel_h': 3,\n",
    "    'kernel_w': 3,\n",
    "    'pad': 0,\n",
    "    'stride': 2,\n",
    "    'in_channel': 3,\n",
    "    'out_channel': 10\n",
    "}\n",
    "in_height = 10\n",
    "in_width = 20\n",
    "\n",
    "out_height = 1+(in_height+2*conv_params['pad']-conv_params['kernel_h'])//conv_params['stride']\n",
    "out_width = 1+(in_width+2*conv_params['pad']-conv_params['kernel_w'])//conv_params['stride']\n",
    "\n",
    "input = np.random.uniform(size=(batch, conv_params['in_channel'], in_height, in_width))\n",
    "out_grad = np.random.uniform(size=(batch, conv_params['out_channel'], out_height, out_width))\n",
    "\n",
    "conv = Convolution(conv_params)\n",
    "check_grads_layer(conv, input, out_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "In the file [nn/layers.py](nn/layers.py), the initialization, forward and backward funtion of the class `Pooling` are shown as below:\n",
    "\n",
    "```python\n",
    "class Pooling(Layer):\n",
    "    def __init__(self, pool_params, name='pooling'):\n",
    "        super(Pooling, self).__init__(name=name)\n",
    "        self.pool_params = pool_params\n",
    "        self.pool = pool(pool_params)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.pool.forward(input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        in_grad = self.pool.backward(out_grad, input)\n",
    "        return in_grad\n",
    "```\n",
    "\n",
    "`pool_params` is a dictionary, containing these parameters:\n",
    "- 'pool_type': The type of pooling, 'max' or 'avg'\n",
    "- 'pool_h': The height of pooling kernel.\n",
    "- 'pool_w': The width of pooling kernel.\n",
    "- 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions.\n",
    "- 'pad': The number of pixels that will be used to zero-pad the input in each x-y direction. **Here pad=2 means adding 2 zeros to the left, right, top and bottom respectively**.\n",
    "\n",
    "You need to implement forward and backward funtion of `pool` class in [nn/operations.py](nn/operations.py), which are called in `Pooling` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward\n",
    "Implement the forward function for `pool` class in the file [nn/operations.py](nn/operations.py).\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error (<1e-6 will be fine):  7.838633230230997e-09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from nn.layers import Pooling\n",
    "from utils.tools import rel_error\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import MaxPooling2D,AveragePooling2D\n",
    "\n",
    "input = np.random.uniform(size=(10, 3, 30, 30))\n",
    "params = { \n",
    "    'pool_type': 'max',\n",
    "    'pool_height': 5,\n",
    "    'pool_width': 5,\n",
    "    'pad': 0,\n",
    "    'stride': 2,\n",
    "}\n",
    "pool = Pooling(params)\n",
    "out = pool.forward(input)\n",
    "\n",
    "keras_pool = Sequential([\n",
    "    MaxPooling2D(pool_size=(params['pool_height'], params['pool_width']),\n",
    "                 strides=params['stride'],\n",
    "                 padding='valid',\n",
    "                 data_format='channels_first',\n",
    "                 input_shape=input.shape[1:])\n",
    "])\n",
    "keras_out = keras_pool.predict(input, batch_size=input.shape[0])\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward\n",
    "Implement the backward function for `pool` class in the file [nn/operations.py](nn/operations.py). **You need to implement max-pooing and avg-pooling according to 'pool_type' in pool_params**\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient to input: correct\n"
     ]
    }
   ],
   "source": [
    "from nn.layers import Pooling\n",
    "import numpy as np\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "batch = 10\n",
    "pool_params={\n",
    "    'pool_type': 'avg',\n",
    "    'pool_height': 2,\n",
    "    'pool_width': 3,\n",
    "    'stride': 2,\n",
    "    'pad': 0\n",
    "}\n",
    "in_height = 10\n",
    "in_width = 10\n",
    "in_channel = 10\n",
    "out_height = 1+(in_height-pool_params['pool_height']+2*pool_params['pad'])//pool_params['stride']\n",
    "out_width = 1+(in_width-pool_params['pool_width']+2*pool_params['pad'])//pool_params['stride']\n",
    "\n",
    "input = np.random.uniform(size=(batch, in_channel, in_height, in_width))\n",
    "out_grads = np.random.uniform(size=(batch, in_channel, out_height, out_width))\n",
    "pool = Pooling(pool_params)\n",
    "check_grads_layer(pool, input, out_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layer\n",
    "Dropout [1] is a technique for regularizing neural networks by randomly setting some features to zero during the forward pass. In this exercise you will implement a dropout layer and modify your fully-connected network to optionally use dropout.\n",
    "\n",
    "[1] Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012\n",
    "\n",
    "In the file `layers.py`, the initialization, forward and backward function of class `Dropout` are shown as below:\n",
    "\n",
    "```python\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, rate, name='dropout', seed=None):\n",
    "        super(Dropout, self).__init__(name=name)\n",
    "        self.rate = rate\n",
    "        self.seed = seed\n",
    "        self.dropout = dropout(rate, self.training, seed)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.dropout.forward(input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        in_grad = self.dropout.backward(out_grad, input)\n",
    "        return in_grad\n",
    "```\n",
    "\n",
    "- `rate`: The probability of setting a neuron to zero\n",
    "- `seed`: seed: int, random seed to sample from input, so as to get mask, which is convenient to check gradients. But for real training, it should be None to make sure to randomly drop neurons\n",
    "\n",
    "You need to implement forward and backward funtion of `dropout` class in [nn/operations.py](nn/operations.py), which are called in `Dropout` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward\n",
    "In the file [nn/operations.py](nn/operations.py), implement the forward function for `dropout` class. Since dropout behaves differently during training and testing, make sure to implement the operation for both modes.  `p` refers to the probability of setting a neuron to zero. We will follow the Caffe convention where we multiply the outputs by `1/(1-p)` during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward\n",
    "In the file [nn/operations.py](nn/operations.py), implement the backward function for `dropout` class. After the implementation, restart jupyter notebook and run the following cell to numerically gradient-check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient to input: correct\n"
     ]
    }
   ],
   "source": [
    "from nn.layers import Dropout\n",
    "\n",
    "import numpy as np\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "rate = 0.1\n",
    "batch = 2\n",
    "height = 10\n",
    "width = 20\n",
    "channel = 10\n",
    "\n",
    "np.random.seed(1234)\n",
    "input = np.random.uniform(size=(batch, channel, height, width))\n",
    "out_grads = np.random.uniform(size=(batch, channel, height, width))\n",
    "\n",
    "dropout = Dropout(rate, seed=1234)\n",
    "dropout.set_mode(True)\n",
    "check_grads_layer(dropout, input, out_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net on full MNIST data\n",
    "By training the `MNISTNet` for one epoch, you should achieve above 90% on test set. You may have to wait about 5 minutes for training to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  48000\n",
      "Number of validation images:  12000\n",
      "Number of testing images:  10000\n",
      "\n",
      "Four examples of training images:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x106b85e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from models.MNISTNet import MNISTNet\n",
    "from nn.loss import SoftmaxCrossEntropy, L2\n",
    "from nn.optimizers import Adam\n",
    "from utils.datasets import MNIST\n",
    "import numpy as np\n",
    "\n",
    "mnist = MNIST()\n",
    "mnist.load()\n",
    "idx = np.random.randint(mnist.num_train, size=4)\n",
    "print('\\nFour examples of training images:')\n",
    "img = mnist.x_train[idx][:,0,:,:]\n",
    "\n",
    "plt.figure(1, figsize=(18, 18))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(img[0])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(img[1])\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(img[2])\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(img[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTNet()\n",
    "loss = SoftmaxCrossEntropy(num_class=10)\n",
    "\n",
    "# define your learning rate sheduler\n",
    "def func(lr, iteration):\n",
    "    if iteration % 1000 ==0:\n",
    "        return lr*0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "adam = Adam(lr=0.0001, decay=0,  sheduler_func = func)\n",
    "l2 = L2(w=0.001) # L2 regularization with lambda=0.001\n",
    "model.compile(optimizer=adam, loss=loss, regularization=l2)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "train_results, val_results, test_results = model.train(\n",
    "    mnist, \n",
    "    train_batch=30, val_batch=1000, test_batch=1000, \n",
    "    epochs=2, \n",
    "    val_intervals=-1, test_intervals=500, print_intervals=100)\n",
    "print('cost:', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(train_results[:,0], train_results[:,1])\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Training accuracy')\n",
    "plt.plot(train_results[:,0], train_results[:,2])\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Testing loss')\n",
    "plt.plot(test_results[:,0], test_results[:, 1])\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Testing accuracy')\n",
    "plt.plot(test_results[:, 0], test_results[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your best MNISTNet!\n",
    "Tweak the hyperparameters and structure of the MNISTNet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your new model and all training codes here, like loading data, defining optimizer and so on\n",
    "\n",
    "from nn.optimizers import Adam,SGD, Adagrad,RMSProp\n",
    "\n",
    "def MNISTmodel():\n",
    "    conv1_params = {\n",
    "        'kernel_h': 3,\n",
    "        'kernel_w': 3,\n",
    "        'pad': 0,\n",
    "        'stride': 1,\n",
    "        'in_channel': 1,\n",
    "        'out_channel': 6\n",
    "    }\n",
    "    pool1_params = {\n",
    "        'pool_type': 'max',\n",
    "        'pool_height': 2,\n",
    "        'pool_width': 2,\n",
    "        'stride': 2,\n",
    "        'pad': 0\n",
    "    }\n",
    "    model = Model()\n",
    "    model.add(Convolution(conv1_params, name='conv1',\n",
    "                          initializer=Gaussian(std=0.001)))\n",
    "    model.add(ReLU(name='relu1'))\n",
    "    model.add(Pooling(pool1_params, name='pooling1'))\n",
    "    model.add(Dropout(ratio=0.1))\n",
    "    model.add(Flatten(name='flatten'))\n",
    "    model.add(FCLayer(512, 256, name='fclayer1',\n",
    "                      initializer=Gaussian(std=0.01)))\n",
    "    model.add(ReLU(name='relu3'))\n",
    "    model.add(Dropout(ratio=0.4))\n",
    "    model.add(FCLayer(256, 10, name='fclayer2',\n",
    "                      initializer=Gaussian(std=0.01)))\n",
    "    return model\n",
    "\n",
    "model = MNISTmodel()\n",
    "loss = SoftmaxCrossEntropy(num_class=10)\n",
    "\n",
    "adam = Adam(lr=0.001, decay=0,  sheduler_func = func)\n",
    "l2 = L2(w=0.001) # L2 regularization with lambda=0.001\n",
    "model.compile(optimizer=adam, loss=loss, regularization=l2)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "train_results, val_results, test_results = model.train(\n",
    "    mnist, \n",
    "    train_batch=60, val_batch=1000, test_batch=1000, \n",
    "    epochs=3, \n",
    "    val_intervals=-1, test_intervals=500, print_intervals=100)\n",
    "print('cost:', time.time()-start)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTmodel()\n",
    "loss = SoftmaxCrossEntropy(num_class=10)\n",
    "\n",
    "sgd = SGD(lr=0.001, momentum=0.1, decay=0, nesterov=False, sheduler_func = func)\n",
    "l2 = L2(w=0.001) # L2 regularization with lambda=0.001\n",
    "model.compile(optimizer=sgd, loss=loss, regularization=l2)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "train_results, val_results, test_results = model.train(\n",
    "    mnist, \n",
    "    train_batch=60, val_batch=1000, test_batch=1000, \n",
    "    epochs=3, \n",
    "    val_intervals=-1, test_intervals=500, print_intervals=100)\n",
    "print('cost:', time.time()-start)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTmodel()\n",
    "loss = SoftmaxCrossEntropy(num_class=10)\n",
    "\n",
    "adagrad = Adagrad(lr=0.001, epsilon=None, decay=0, sheduler_func=func)\n",
    "l2 = L2(w=0.001) # L2 regularization with lambda=0.001\n",
    "model.compile(optimizer=adagrad, loss=loss, regularization=l2)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "train_results, val_results, test_results = model.train(\n",
    "    mnist, \n",
    "    train_batch=60, val_batch=1000, test_batch=1000, \n",
    "    epochs=3, \n",
    "    val_intervals=-1, test_intervals=500, print_intervals=100)\n",
    "print('cost:', time.time()-start)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTmodel()\n",
    "loss = SoftmaxCrossEntropy(num_class=10)\n",
    "\n",
    "rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0, sheduler_func=None)\n",
    "l2 = L2(w=0.001) # L2 regularization with lambda=0.001\n",
    "model.compile(optimizer=rmsprop, loss=loss, regularization=l2)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "train_results, val_results, test_results = model.train(\n",
    "    mnist, \n",
    "    train_batch=60, val_batch=1000, test_batch=1000, \n",
    "    epochs=3, \n",
    "    val_intervals=-1, test_intervals=500, print_intervals=100)\n",
    "print('cost:', time.time()-start)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description and Results\n",
    "\n",
    "#### Default MNIST\n",
    "\n",
    "Using the default model with Adam as optimizer, we get an accuracy rate of 90% on the test data and 93% on the validation data.\n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "We modified the default MNISTNet ; at first, one more convolution layer was added. However, that did not improve the model accuracy by a significant amount.\n",
    "We then removed one convolution layer and added two dropouts. We also changed the parameters of the Fully Connected layer. Adding dropout helps in regularization and reduces overfitting.\n",
    "\n",
    "#### Model Hyperparameters\n",
    "\n",
    "1. We first increased the training batch size from 30 to 45; and then to 60. 60 gave us the highest test accuracy.\n",
    "2. We also increased number of epochs from 2 to 3, to pass the entire dataset thrice through the entire neural network (3 forward passes and 3 backward passes). However, this is also a time-consuming operation.\n",
    "\n",
    "#### Hyperparameters of optimizers\n",
    "\n",
    "We tried all different optimizers, and to be able to compare them with each other, we used the same hyperparameters (where we could).\n",
    "\n",
    "| Optimizer-> | Adam | SGD | Adagrad | RMSprop|\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Learning Rate | 0.01 | 0.01 | 0.01 | 0.01 |\n",
    "| Decay | 0 | 0 | 0 | 0 |\n",
    "| Other | None | momentum=0.1 | None | rho=0.9 |\n",
    "\n",
    "#### Final Accuracy\n",
    "\n",
    "| Optimizer-> | Adam | SGD | Adagrad | RMSprop|\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Accuracy | 0 | 0 | 0 | 0 |\n",
    "\n",
    "We see that Adam performs the best and SGD performs the worst. Empirically, Adam has been shown to work well in practice, and is robust to changes in hyperparameters too.\n",
    "For the other optimizers, more tuning is needed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marking Scheme\n",
    "\n",
    "Marking scheme is shown below:\n",
    "- 4 marks for `Adam` update function\n",
    "- 5 marks for `conv` forward and backward function\n",
    "- 4 marks for `pool` forward and backward function\n",
    "- 3 marks for `dropout` forward and backward function\n",
    "- 3 marks for tuning your best MNISTNet\n",
    "- 1 marks for your submission format\n",
    "\n",
    "We will run multiple test cases to check the correctness of the implementation for `Adam`, `conv`, `pool`, `dropout`. The submission format is shown below.\n",
    "\n",
    "**DO NOT** use external libraries like Tensorflow, keras and Pytorch in your implementation (*.py files). **DO NOT** copy the code from the internet, e.g. github. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final submission instructions\n",
    "Please submit the following:\n",
    "\n",
    "1) Your codes in a folder named `codes`, and keep the structure of all files in this folder the same as what we have provided. \n",
    "\n",
    "**ASSIGNMENT DEADLINE: 11 Mar 2019 (MON) 17:00**\n",
    "\n",
    "Do not include the `data` folder as it takes up substantial memory. Please zip up the following folders under a folder named with your NUSNET ID: eg. `e0123456g.zip` and submit the zipped folder to LumiNUS/Files/Assignment 1 Submission. The structure should be like this after unzip:\n",
    "\n",
    "```bash\n",
    "e0123456g/\n",
    "    codes/\n",
    "        models/\n",
    "            ...\n",
    "        nn/\n",
    "            ...\n",
    "        utils/\n",
    "            ...\n",
    "        main.ipynb\n",
    "        README.MD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam\n",
    "http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "#### Forward and Backward Pass for conv and pooling\n",
    "https://lanstonchu.wordpress.com/2018/09/01/convolutional-neural-network-cnn-backward-propagation-of-the-pooling-layers/\n",
    "\n",
    "https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199\n",
    "\n",
    "https://github.com/BetaCollins/CNN-build-from-scratch/\n",
    "\n",
    "https://github.com/fanghao6666/\n",
    "\n",
    "https://wiseodd.github.io/techblog/2016/07/16/convnet-conv-layer/\n",
    "\n",
    "http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "http://cs231n.stanford.edu/handouts/linear-backprop.pdf\n",
    "\n",
    "https://knet.readthedocs.io/en/latest/cnn.html\n",
    "\n",
    "https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c\n",
    "\n",
    "#### Dropout\n",
    "https://wiseodd.github.io/techblog/2016/06/25/dropout/\n",
    "\n",
    "#### MNIST training\n",
    "https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d\n"
   ]
  }
 ],
 "metadata": {
  "@deathbeds/jupyterlab-fonts": {
   "fontLicenses": {
    "Anonymous Pro Regular": {
     "holders": [
      "Copyright (c) 2009, Mark Simonson (http://www.ms-studio.com, mark@marksimonson.com), with Reserved Font Name Anonymous Pro Minus."
     ],
     "name": "SIL Open Font License 1.1",
     "spdx": "OFL-1.1",
     "text": "Copyright (c) 2009, Mark Simonson (http://www.ms-studio.com, mark@marksimonson.com),\nwith Reserved Font Name Anonymous Pro Minus.\n\nThis Font Software is licensed under the SIL Open Font License, Version 1.1.\nThis license is copied below, and is also available with a FAQ at:\nhttp://scripts.sil.org/OFL\n\n\n-----------------------------------------------------------\nSIL OPEN FONT LICENSE Version 1.1 - 26 February 2007\n-----------------------------------------------------------\n\nPREAMBLE\nThe goals of the Open Font License (OFL) are to stimulate worldwide\ndevelopment of collaborative font projects, to support the font creation\nefforts of academic and linguistic communities, and to provide a free and\nopen framework in which fonts may be shared and improved in partnership\nwith others.\n\nThe OFL allows the licensed fonts to be used, studied, modified and\nredistributed freely as long as they are not sold by themselves. The\nfonts, including any derivative works, can be bundled, embedded,\nredistributed and/or sold with any software provided that any reserved\nnames are not used by derivative works. The fonts and derivatives,\nhowever, cannot be released under any other type of license. The\nrequirement for fonts to remain under this license does not apply\nto any document created using the fonts or their derivatives.\n\nDEFINITIONS\n\"Font Software\" refers to the set of files released by the Copyright\nHolder(s) under this license and clearly marked as such. This may\ninclude source files, build scripts and documentation.\n\n\"Reserved Font Name\" refers to any names specified as such after the\ncopyright statement(s).\n\n\"Original Version\" refers to the collection of Font Software components as\ndistributed by the Copyright Holder(s).\n\n\"Modified Version\" refers to any derivative made by adding to, deleting,\nor substituting -- in part or in whole -- any of the components of the\nOriginal Version, by changing formats or by porting the Font Software to a\nnew environment.\n\n\"Author\" refers to any designer, engineer, programmer, technical\nwriter or other person who contributed to the Font Software.\n\nPERMISSION & CONDITIONS\nPermission is hereby granted, free of charge, to any person obtaining\na copy of the Font Software, to use, study, copy, merge, embed, modify,\nredistribute, and sell modified and unmodified copies of the Font\nSoftware, subject to the following conditions:\n\n1) Neither the Font Software nor any of its individual components,\nin Original or Modified Versions, may be sold by itself.\n\n2) Original or Modified Versions of the Font Software may be bundled,\nredistributed and/or sold with any software, provided that each copy\ncontains the above copyright notice and this license. These can be\nincluded either as stand-alone text files, human-readable headers or\nin the appropriate machine-readable metadata fields within text or\nbinary files as long as those fields can be easily viewed by the user.\n\n3) No Modified Version of the Font Software may use the Reserved Font\nName(s) unless explicit written permission is granted by the corresponding\nCopyright Holder. This restriction only applies to the primary font name as\npresented to the users.\n\n4) The name(s) of the Copyright Holder(s) or the Author(s) of the Font\nSoftware shall not be used to promote, endorse or advertise any\nModified Version, except to acknowledge the contribution(s) of the\nCopyright Holder(s) and the Author(s) or with their explicit written\npermission.\n\n5) The Font Software, modified or unmodified, in part or in whole,\nmust be distributed entirely under this license, and must not be\ndistributed under any other license. The requirement for fonts to\nremain under this license does not apply to any document created\nusing the Font Software.\n\nTERMINATION\nThis license becomes null and void if any of the above conditions are\nnot met.\n\nDISCLAIMER\nTHE FONT SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT\nOF COPYRIGHT, PATENT, TRADEMARK, OR OTHER RIGHT. IN NO EVENT SHALL THE\nCOPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nINCLUDING ANY GENERAL, SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL\nDAMAGES, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF THE USE OR INABILITY TO USE THE FONT SOFTWARE OR FROM\nOTHER DEALINGS IN THE FONT SOFTWARE.\n"
    }
   },
   "fonts": {
    "Anonymous Pro Regular": [
     {
      "fontFamily": "'Anonymous Pro Regular'",
      "src": "url('data:font/woff2;charset=utf-8;base64,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') format('woff2')"
     }
    ]
   },
   "styles": {
    ":root": {
     "--jp-code-font-family": "'Anonymous Pro Regular'",
     "--jp-code-font-size": "16px",
     "--jp-content-font-size1": "16px"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
